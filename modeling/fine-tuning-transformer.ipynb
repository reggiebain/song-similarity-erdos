{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine Tuning Transformer for Audio Similarity Analysis\n","- Load and fine-tune [Audio Spectrogram Transformer ](https://arxiv.org/abs/2104.01778) from [HuggingFace](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593) for use in song similarity. \n","- Create dataset class for feeding spectrograms into Transformer architecture\n","- Train on the triplet loss to\n","    1. Minimize euclidean distance between anchor positive\n","    2. Maximize euclidean distance between anchor negative\n","- Output model weights for deployment on songs with real covers     "]},{"cell_type":"markdown","metadata":{},"source":["#### Import Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T01:18:31.837800Z","iopub.status.busy":"2024-08-27T01:18:31.837528Z","iopub.status.idle":"2024-08-27T01:19:00.833876Z","shell.execute_reply":"2024-08-27T01:19:00.832966Z","shell.execute_reply.started":"2024-08-27T01:18:31.837769Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import librosa\n","import librosa.display\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","from dotenv import dotenv_values \n","import pickle as pkl\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optim\n","\n","from scipy.spatial.distance import euclidean\n","from sklearn.model_selection import train_test_split\n","from transformers import Wav2Vec2Model\n","from transformers import AutoProcessor, AutoModel, ASTModel, AutoFeatureExtractor, AutoModelForAudioClassification"]},{"cell_type":"markdown","metadata":{},"source":["#### Create Dataset Class"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T01:37:47.142978Z","iopub.status.busy":"2024-08-27T01:37:47.142271Z","iopub.status.idle":"2024-08-27T01:37:47.158622Z","shell.execute_reply":"2024-08-27T01:37:47.157733Z","shell.execute_reply.started":"2024-08-27T01:37:47.142935Z"},"trusted":true},"outputs":[],"source":["class SpectrogramDataset(Dataset):\n","    def __init__(self, file_paths, transform=False, sr=22050, target_sr=16000, n_mels=128):\n","        self.file_paths = file_paths\n","        self.data_index = self._build_index()\n","        self.sr = sr\n","        self.target_sr=target_sr\n","        self.n_mels = n_mels\n","        self.transform = transform\n","        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n","\n","    def _build_index(self):\n","        index = []\n","        for file_idx, file_path in enumerate(self.file_paths):\n","            with open(file_path, 'rb') as f:\n","                data = pkl.load(f)\n","                for i in range(len(data)):\n","                    index.append((file_idx, i))\n","        return index\n","\n","    def _get_log_mel_spectrogram(self, y):\n","        # Convert to mel spectrogram\n","        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n","        # Convert to log scale\n","        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","        log_mel_spectrogram = torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)\n","        return log_mel_spectrogram\n","\n","    def __len__(self):\n","        return len(self.data_index)\n","\n","    def __getitem__(self, idx):\n","        file_idx, data_idx = self.data_index[idx]\n","        file_path = self.file_paths[file_idx]\n","\n","        with open(file_path, 'rb') as f:\n","            data = pkl.load(f)\n","        \n","        row = data.iloc[data_idx]\n","        anchor = row['processed_audio'][0]  # (y, sr)\n","        positive = row['augmented_audio'][0]\n","        negative = row['diff_processed_audio'][0]\n","        \n","        anchor = librosa.resample(anchor, orig_sr=self.sr, target_sr=self.target_sr)\n","        positive = librosa.resample(positive, orig_sr=self.sr, target_sr=self.target_sr)\n","        negative = librosa.resample(negative, orig_sr=self.sr, target_sr=self.target_sr)\n","        \n","        #inputs = self.feature_extractor(audio_data, sampling_rate=self.sampling_rate, return_tensors=\"pt\")\n","        anchor = self.feature_extractor(anchor, sampling_rate=self.target_sr, return_tensors='pt')\n","        positive = self.feature_extractor(positive, sampling_rate=self.target_sr, return_tensors='pt')\n","        negative = self.feature_extractor(negative, sampling_rate=self.target_sr, return_tensors='pt')\n","        \n","        anchor_mel = anchor[\"input_values\"].squeeze(0)\n","        positive_mel = positive['input_values'].squeeze(0)\n","        negative_mel = negative['input_values'].squeeze(0)\n","        \n","        #print(f\"{anchor=}\")\n","        # Convert to log mel spectrograms\n","        #anchor_mel = self._get_log_mel_spectrogram(anchor)\n","        #positive_mel = self._get_log_mel_spectrogram(positive)\n","        #negative_mel = self._get_log_mel_spectrogram(negative)\n","        # Pass in audio from librosa \n","        #anchor_mel = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n","        #positive_mel = torch.tensor(positive, dtype=torch.float32).unsqueeze(0)\n","        #negative_mel = torch.tensor(negative, dtype=torch.float32).unsqueeze(0)\n","        \n","        # Apply any transformations\n","        if self.transform:\n","            anchors = self.transform(anchors)\n","            positives = self.transform(positives)\n","            negatives = self.transform(negatives)\n","\n","        return anchor_mel, positive_mel, negative_mel\n"]},{"cell_type":"markdown","metadata":{},"source":["### Split Data and Instantiate Dataset Class and DataLoaders"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T01:37:47.559113Z","iopub.status.busy":"2024-08-27T01:37:47.558334Z","iopub.status.idle":"2024-08-27T01:37:55.678766Z","shell.execute_reply":"2024-08-27T01:37:55.677721Z","shell.execute_reply.started":"2024-08-27T01:37:47.559077Z"},"trusted":true},"outputs":[],"source":["file_paths = [f'/kaggle/input/augmented-audio-10k/batch_{i}_augmented.pkl' for i in range(1,10,1)]\n","\n","# Split the files instaed of actual data into training/val\n","train_files, val_files = train_test_split(file_paths, test_size=0.2, random_state=123)\n","\n","# Instantiate Dataset Classes\n","train_dataset = SpectrogramDataset(train_files)\n","val_dataset = SpectrogramDataset(val_files)\n","\n","# Declare dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)#, drop_last=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T01:37:55.680715Z","iopub.status.busy":"2024-08-27T01:37:55.680365Z","iopub.status.idle":"2024-08-27T01:37:56.403227Z","shell.execute_reply":"2024-08-27T01:37:56.402213Z","shell.execute_reply.started":"2024-08-27T01:37:55.680683Z"},"trusted":true},"outputs":[],"source":["#test = train_loader.dataset[0][0]"]},{"cell_type":"markdown","metadata":{},"source":["### Delcaring the Model\n","- Define architecture: standard CNN with batch norms and pooling to create 128 dim embeddings\n","- Choose loss function, optimizer, device, etc.\n","**Note:**\n","- Wave2Vec2Model expects input in shape [batch size, sequence length] or [batch size, channels, sequence length]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T01:41:06.260564Z","iopub.status.busy":"2024-08-27T01:41:06.259861Z","iopub.status.idle":"2024-08-27T01:41:06.268008Z","shell.execute_reply":"2024-08-27T01:41:06.267121Z","shell.execute_reply.started":"2024-08-27T01:41:06.260526Z"},"trusted":true},"outputs":[],"source":["class AudioSpecTransformerModel(torch.nn.Module):\n","    def __init__(self, pretrained_model_name=\"MIT/ast-finetuned-audioset-10-10-0.4593\", embedding_dim=128, dropout_rate=0.5):\n","        super(AudioSpecTransformerModel, self).__init__()\n","        \n","        #self.model = AutoModel.from_pretrained(pretrained_model_name)\n","        #self.model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n","        self.model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n","        #self.model = Wav2Vec2Model.from_pretrained(pretrained_model_name)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.fc = torch.nn.Linear(self.model.config.hidden_size, embedding_dim)\n","\n","    def forward(self, x):\n","        #print(f\"Input shape to model in forward: {x.shape}\")\n","        #print(self.model)\n","        #print(f\"{x.view(x.size(0), -1).shape=}\")\n","        #print(f\"{x.view(x.size(0), -1)=}\")\n","        #print(f\"{self.model(x).shape=}\")\n","        #print(f\"{self.model(x)=}\")\n","        #output = self.model(x).last_hidden_state\n","        #print(f\"Output Shape After Last Hidden state: {output.shape}\")\n","        #print(f\"{output=}\")\n","        #x = x.mean(dim=1)\n","        #embeddings = self.fc(x)\n","        #print(f\"Embeddings Shape: {embeddings.shape}\")\n","        #return F.normalize(embeddings, p=2, dim=1)\n","        # Pass through the transformer model\n","        outputs = self.model(x).last_hidden_state\n","        \n","        # Apply the final fully connected layer to get the embeddings\n","        x = self.fc(outputs[:, 0, :])  # Use the [CLS] token embedding\n","        \n","        return F.normalize(x, p=2, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Choose model, loss, and optimizer\n","model = AudioSpecTransformerModel()\n","criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n","optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n","\n","# Declare losses/accuracies\n","train_losses = []\n","val_losses = []\n","baseline_losses = []\n","\n","num_epochs = 15\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Training Model"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T01:38:33.980488Z","iopub.status.busy":"2024-08-27T01:38:33.980099Z","iopub.status.idle":"2024-08-27T01:38:33.988098Z","shell.execute_reply":"2024-08-27T01:38:33.986887Z","shell.execute_reply.started":"2024-08-27T01:38:33.980449Z"},"trusted":true},"outputs":[],"source":["def save_checkpoint(model, optimizer, epoch, file_path=\"distilhubert_checkpoint.pth\"):\n","    checkpoint = {\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'epoch': epoch,\n","    }\n","    torch.save(checkpoint, file_path)\n","    print(f\"Checkpoint saved at epoch {epoch} to {file_path}\")\n","\n","def load_checkpoint(model, optimizer, file_path=\"/kaggle/working/distilhubert_checkpoint.pth\"):\n","    if os.path.isfile(file_path):\n","        checkpoint = torch.load(file_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch'] + 1\n","        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n","        return start_epoch\n","    else:\n","        print(\"No checkpoint found. Starting from scratch.\")\n","        return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_path = \"/kaggle/working/distilhubert_checkpoint.pth\"\n","save_frequency = 5  # Save every 5 epochs\n","\n","# Load checkpoint if available\n","start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n","\n","# Loop over epochs\n","for epoch in range(start_epoch, num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    running_train_loss = 0.0\n","    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\", unit=\"batch\")\n","\n","    # Loop over batches using dataloaders\n","    for anchors, positives, negatives in train_loader:\n","        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n","        # current shape is [Batch size, channels, height, width]\n","        optimizer.zero_grad()\n","\n","        anchor_embeddings = model(anchors)\n","        positive_embeddings = model(positives)\n","        negative_embeddings = model(negatives)\n","        \n","        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_train_loss += loss.item() * anchors.size(0)\n","        pbar.update(1)\n","          \n","    train_loss = running_train_loss / len(train_loader.dataset)\n","    train_losses.append(train_loss)\n","\n","    # Turn on validation/eval mode\n","    model.eval()\n","    running_val_loss = 0.0 \n","    running_baseline_loss = 0.0   \n","    val_pbar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\")\n","    \n","    # Turn off gradient updates since we're in validation\n","    with torch.no_grad():\n","        # Batch loop \n","        for anchors, positives, negatives in tqdm(val_loader):\n","            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n","            \n","            anchor_embeddings = model(anchors)\n","            positive_embeddings = model(positives)\n","            negative_embeddings = model(negatives)\n","            \n","            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n","\n","            # Add to running val loss\n","            running_val_loss += loss.item() * anchors.size(0)\n","            \n","            # baseline loss\n","            baseline_loss = criterion(F.normalize(anchors), \n","                                               F.normalize(positives), \n","                                               F.normalize(negatives)).item()\n","            running_baseline_loss += baseline_loss*anchors.size(0)\n","            \n","            # Update the validation progress bar\n","            val_pbar.update(1)\n","    \n","    # Calculate average validation loss over the entire dataset\n","    val_loss = running_val_loss / len(val_loader.dataset)\n","    val_losses.append(val_loss)\n","    # Do the same for the baseline\n","    baseline_avg_loss = running_baseline_loss / len(val_loader.dataset)\n","    baseline_losses.append(baseline_avg_loss)\n","    \n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Baseline Loss: {baseline_avg_loss:.4f}\")\n","    \n","    if (epoch+1)% save_frequency == 0:\n","        save_checkpoint(model, optimizer, epoch, checkpoint_path)\n","    \n","    with open('training_logs.pkl', 'wb') as f:\n","        pkl.dump((train_losses, val_losses), f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:03:36.019206Z","iopub.status.idle":"2024-08-26T17:03:36.019598Z","shell.execute_reply":"2024-08-26T17:03:36.019417Z","shell.execute_reply.started":"2024-08-26T17:03:36.019397Z"},"trusted":true},"outputs":[],"source":["# Plot loss curves for training\n","epochs = range(1, num_epochs + 1)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","plt.plot(epochs, baseline_losses, label='Baseline Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('distilhubert-loss-plot.png')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:03:36.020877Z","iopub.status.idle":"2024-08-26T17:03:36.021330Z","shell.execute_reply":"2024-08-26T17:03:36.021121Z","shell.execute_reply.started":"2024-08-26T17:03:36.021095Z"},"trusted":true},"outputs":[],"source":["# Save just the model weights (recommended apparently for portability/compatibility)\n","torch.save(model.state_dict(), 'distilhubert_weights.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-08-26T17:03:36.022504Z","iopub.status.idle":"2024-08-26T17:03:36.022941Z","shell.execute_reply":"2024-08-26T17:03:36.022729Z","shell.execute_reply.started":"2024-08-26T17:03:36.022708Z"},"trusted":true},"outputs":[],"source":["# Save the entire model so we can use it for deployment\n","torch.save(model, 'distilhubert_model.pth')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5535463,"sourceId":9162188,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
