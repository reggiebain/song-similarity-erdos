{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9020098,"sourceType":"datasetVersion","datasetId":5435569},{"sourceId":9219897,"sourceType":"datasetVersion","datasetId":5539947}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetuning Transformer embeddings\n- Create Siamese network with transformer archetecture\n- Finetune pretrained models to determine song similarity","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\n#from dotenv import dotenv_values \n#import spotipy\n#from spotipy.oauth2 import SpotifyClientCredentials\nimport pickle as pkl\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import  HubertForSequenceClassification, Wav2Vec2BertModel, Wav2Vec2BertForSequenceClassification, AutoProcessor, AutoFeatureExtractor","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:55:45.419105Z","iopub.execute_input":"2024-08-24T17:55:45.419469Z","iopub.status.idle":"2024-08-24T17:56:06.976672Z","shell.execute_reply.started":"2024-08-24T17:55:45.419439Z","shell.execute_reply":"2024-08-24T17:56:06.975727Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-24 17:55:55.739185: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-24 17:55:55.739305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-24 17:55:55.908299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Dataset class for the Wav2Vec2Bert models\nclass SpectrogramDataset(Dataset):\n\n    def __init__(self, audio, sr=16000, n_mels=128):\n\n        self.audio = audio\n        self.sr = sr\n        self.n_mels = n_mels\n    \n    \n    def __len__(self):\n        return self.audio.shape[0]\n    \n    def __getitem__(self, idx):\n\n        # Load audio data and select idx'th example and get [0] to get audio from (y, sr) tuple\n        anchor = self.audio['processed_audio'].values[idx][0]#.astype(np.float16)\n        positive = self.audio['augmented_audio'].values[idx][0]#.astype(np.float16)\n        negative = self.audio['diff_processed_audio'].values[idx][0]#.astype(np.float16)\n\n        # Compute mel spectrograms\n        anchor_mel = self._process_audio(anchor)\n        positive_mel = self._process_audio(positive)\n        negative_mel = self._process_audio(negative)\n\n        return anchor_mel, positive_mel, negative_mel\n    \n    # Convert raw audio to mel spectrogram\n    def _process_audio(self, y):\n        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n        mel_spectrogram_db = torch.tensor(mel_spectrogram_db, dtype=torch.float32).unsqueeze(0)\n\n        return mel_spectrogram_db","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:23.453869Z","iopub.execute_input":"2024-08-24T17:56:23.454609Z","iopub.status.idle":"2024-08-24T17:56:23.464398Z","shell.execute_reply.started":"2024-08-24T17:56:23.454575Z","shell.execute_reply":"2024-08-24T17:56:23.463320Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Dataset class for the Hubert model\nclass RawAudioDataset(Dataset):\n\n    def __init__(self, audio):\n\n        self.audio = audio\n        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"superb/hubert-base-superb-ks\")\n    \n    def __len__(self):\n        return self.audio.shape[0]\n    \n    def __getitem__(self, idx):\n\n        # Load audio data and select idx'th example and get [0] to get audio from (y, sr) tuple\n        anchor = self.audio['processed_audio'].values[idx][0]#.astype(np.float16)\n        positive = self.audio['augmented_audio'].values[idx][0]#.astype(np.float16)\n        negative = self.audio['diff_processed_audio'].values[idx][0]#.astype(np.float16)\n    \n        anchor = self.feature_extractor(anchor, sampling_rate=16000, return_tensors=\"pt\")\n        positive = self.feature_extractor(positive, sampling_rate=16000, return_tensors=\"pt\")\n        negative = self.feature_extractor(negative, sampling_rate=16000, return_tensors=\"pt\")\n\n        return anchor, positive, negative\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:24.489595Z","iopub.execute_input":"2024-08-24T17:56:24.490506Z","iopub.status.idle":"2024-08-24T17:56:24.501561Z","shell.execute_reply.started":"2024-08-24T17:56:24.490464Z","shell.execute_reply":"2024-08-24T17:56:24.500464Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Use the Wav2VecBertModel from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/wav2vec2-bert\n# with an added fully connected layer to create embedding for Siamese network\n\nclass W2VBertModelEmbedding(nn.Module):\n    def __init__(self, input_len = 313, embedding_dim=128):\n        super(W2VBertModelEmbedding, self).__init__()\n\n        # Load pre-trained Wav2Vec model\n        self.model = Wav2Vec2BertModel.from_pretrained(\"hf-audio/wav2vec2-bert-CV16-en\")\n        self.model.feature_projection.layer_norm = nn.LayerNorm((input_len,), eps=1e-05, elementwise_affine=True)\n        self.model.feature_projection.projection = nn.Linear(in_features=input_len, out_features=1024, bias=True)\n        \n        # Add a fully connected layer to project the hidden states to the desired embedding dimension\n        self.fc = nn.Linear(self.model.config.hidden_size, embedding_dim)\n\n    def forward(self, x):\n        # Extract the last hidden state from the DistilHuBERT model\n        x = self.model(x).last_hidden_state\n        \n        # Apply the fully connected layer to reduce the dimension\n        x = self.fc(x.mean(dim=1))  # Take mean over the time dimension\n        \n        # Normalize the output embeddings\n        return F.normalize(x, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:25.273585Z","iopub.execute_input":"2024-08-24T17:56:25.273956Z","iopub.status.idle":"2024-08-24T17:56:25.282918Z","shell.execute_reply.started":"2024-08-24T17:56:25.273927Z","shell.execute_reply":"2024-08-24T17:56:25.281776Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Use only the encoder from Wav2VecBertModel from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/wav2vec2-bert\n# with an added fully connected layer to create embedding for Siamese network\n\nclass W2VBertEncoderEmbedding(nn.Module):\n    def __init__(self, input_len = 313, embedding_dim=128):\n        super(W2VBertEncoderEmbedding, self).__init__()\n\n        # Load pre-trained Wav2Vec model\n        self.feature_proj = Wav2Vec2BertModel.from_pretrained(\"hf-audio/wav2vec2-bert-CV16-en\").feature_projection\n        self.encoder = Wav2Vec2BertModel.from_pretrained(\"hf-audio/wav2vec2-bert-CV16-en\").encoder\n        self.feature_proj.layer_norm = nn.LayerNorm((input_len,), eps=1e-05, elementwise_affine=True)\n        self.feature_proj.projection = nn.Linear(in_features=input_len, out_features=1024, bias=True)\n        \n        # Add a fully connected layer to project the hidden states to the desired embedding dimension\n        self.fc = nn.Linear(self.encoder.config.hidden_size, embedding_dim)\n\n    def forward(self, x):\n        # Extract the last hidden state from the DistilHuBERT model\n        x = self.feature_proj(x)[0]\n        x = self.encoder(x).last_hidden_state\n        \n        # Apply the fully connected layer to reduce the dimension\n        x = self.fc(x.mean(dim=1))  # Take mean over the time dimension\n        \n        # Normalize the output embeddings\n        return F.normalize(x, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:26.044289Z","iopub.execute_input":"2024-08-24T17:56:26.045115Z","iopub.status.idle":"2024-08-24T17:56:26.055743Z","shell.execute_reply.started":"2024-08-24T17:56:26.045073Z","shell.execute_reply":"2024-08-24T17:56:26.054807Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Use the Wav2Vec2BertForSequenceClassification model from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification\n# to create embedding for Siamese network\nclass W2VBSeqEmbedding(nn.Module):\n    def __init__(self, input_len = 313, embedding_dim=128):\n        super(W2VBSeqEmbedding, self).__init__()\n\n        # Load pre-trained Wav2Vec model\n        self.model = Wav2Vec2BertForSequenceClassification.from_pretrained(\"facebook/w2v-bert-2.0\")\n        self.model.wav2vec2_bert.feature_projection.layer_norm = nn.LayerNorm((input_len,), eps=1e-05, elementwise_affine=True)\n        self.model.wav2vec2_bert.feature_projection.projection = nn.Linear(in_features=input_len, out_features=1024, bias=True)\n        \n        # Add a fully connected layer to project the hidden states to the desired embedding dimension\n        self.model.classifier = nn.Linear(self.model.classifier.in_features, embedding_dim)\n\n    def forward(self, x):\n        # Extract the last hidden state from the DistilHuBERT model\n        x = self.model(x).logits#.last_hidden_state\n        \n        # Apply the fully connected layer to reduce the dimension\n        #x = self.fc(x.mean(dim=1))  # Take mean over the time dimension\n        \n        # Normalize the output embeddings\n        return F.normalize(x, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:26.797654Z","iopub.execute_input":"2024-08-24T17:56:26.798018Z","iopub.status.idle":"2024-08-24T17:56:26.806402Z","shell.execute_reply.started":"2024-08-24T17:56:26.797991Z","shell.execute_reply":"2024-08-24T17:56:26.805221Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Use the HubertForSequenceClassification model from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/hubert#transformers.HubertForSequenceClassification\n# to create embedding for Siamese network\n\nclass HubertEmbedding(nn.Module):\n    def __init__(self, input_len = 313, embedding_dim=128):\n        super(HubertEmbedding, self).__init__()\n        \n        # Load pre-trained Hubert model\n        self.model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-base-superb-ks\")\n        \n        # Add a fully connected layer to project the hidden states to the desired embedding dimension\n        self.model.classifier = nn.Linear(self.model.classifier.in_features, embedding_dim)\n\n    def forward(self, x):\n        # Extract the last hidden state from the DistilHuBERT model\n        x = self.model(x.input_values.squeeze(1)).logits#.last_hidden_state\n        \n        # Apply the fully connected layer to reduce the dimension\n        #x = self.fc(x.mean(dim=1))  # Take mean over the time dimension\n        \n        # Normalize the output embeddings\n        return F.normalize(x, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:27.525071Z","iopub.execute_input":"2024-08-24T17:56:27.525423Z","iopub.status.idle":"2024-08-24T17:56:27.532434Z","shell.execute_reply.started":"2024-08-24T17:56:27.525397Z","shell.execute_reply":"2024-08-24T17:56:27.531582Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define pretrained resnet from Torch Vision resnet 18\nclass ResNetEmbedding(nn.Module):\n    def __init__(self, embedding_dim=128, dropout_rate=0.8):\n        # get resnet super class\n        super(ResNetEmbedding, self).__init__()\n        self.resnet = models.resnet18(weights='DEFAULT')\n        # Change structure of first layer to take non RGB images, rest of params same as default\n        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.dropout = nn.Dropout(p=dropout_rate)\n        # Set the last fully connected to a set dimension \"embedding_dim\" instead of default 1000\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embedding_dim)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        return F.normalize(x, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:28.321177Z","iopub.execute_input":"2024-08-24T17:56:28.321561Z","iopub.status.idle":"2024-08-24T17:56:28.329028Z","shell.execute_reply.started":"2024-08-24T17:56:28.321533Z","shell.execute_reply":"2024-08-24T17:56:28.328006Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Make sure the dataset has 'processed_audio', 'augmented_audio', and 'diff_processed_audio' columns\ndef make_train_loaders(model, dir_path = '/kaggle/input/augmented-music/combined_batch_augmented.pkl', train_batch_size = 16, val_batch_size = 16):\n\n    '''\n        Creates DataLoaders to be used in training and validation\n\n        Arguments: \n            model -- model to be trained\n            dir_path -- path to dataset\n            spec_or_raw -- string, must be 'spec' or 'raw' depending on wether the data should be prepared as a spectrogram or as raw audio data\n            train(val)_batch_size -- size of training (validation) batch sizes\n        Returns:\n            train(val)_loader -- DataLoader of training (validation) samples\n    '''\n    assert isinstance(model, (ResNetEmbedding, W2VBertModelEmbedding,W2VBSeqEmbedding, HubertEmbedding)), 'model must be of class type ResNetEmbedding, W2VBertModelEmbedding,W2VBSeqEmbedding, or HubertEmbedding'\n\n    audio_data = pd.read_pickle(dir_path).iloc[:1000]\n\n    assert 'processed_audio' and 'augmented_audio' and 'diff_processed_audio' in audio_data.columns, 'DataFrame must contain columns \\'processed_audio\\', \\'augmented_audio\\', and \\'diff_processed_audio\\''\n\n    # Split the data into training and validation sets\n    train_data, val_data = train_test_split(audio_data, test_size=0.2, random_state=123)\n\n    try:\n        if isinstance(model, (ResNetEmbedding, W2VBertModelEmbedding,W2VBSeqEmbedding)):\n            train_dataset = SpectrogramDataset(train_data)\n            val_dataset = SpectrogramDataset(val_data)\n        elif isinstance(model, HubertEmbedding):\n            train_dataset = RawAudioDataset(train_data)\n            val_dataset = RawAudioDataset(val_data)\n    except NameError:\n        None\n\n\n    # Define dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n\n    return train_loader, val_loader","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:50.358544Z","iopub.execute_input":"2024-08-24T17:56:50.359356Z","iopub.status.idle":"2024-08-24T17:56:50.370970Z","shell.execute_reply.started":"2024-08-24T17:56:50.359317Z","shell.execute_reply":"2024-08-24T17:56:50.369972Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Freeze layers for finetuning\ndef setup_model(embedding):\n    '''\n    Freeze all but the last few layers of the model.\n    Argument:\n        embedding -- model, must be one of the embedding types defined above\n    Returns:\n        None\n    '''\n    try:\n        if isinstance(embedding, ResNetEmbedding):\n            # Freeze all the layers\n            for param in embedding.resnet.parameters():\n                param.requires_grad = False\n\n            # Turn back on last residual block\n            for param in embedding.resnet.layer4.parameters():\n                param.requires_grad = True\n\n            # Turn back on fully connected layer\n            for param in embedding.resnet.fc.parameters():\n                param.requires_grad = True\n    except NameError:\n        None\n\n    try:\n        if isinstance(embedding, W2VBertModelEmbedding):\n            # Freeze all the layers\n            for param in embedding.model.parameters():\n                param.requires_grad = False\n\n            # Turn back on adapter\n            for param in embedding.model.adapter.parameters():\n                param.requires_grad = True\n    except NameError:\n        None\n\n    try:\n        if isinstance(embedding, W2VBSeqEmbedding):\n\n            # Freeze all the layers\n            for param in embedding.model.parameters():\n                param.requires_grad = False\n\n            # Turn back on last feed forward layer\n            for param in embedding.model.wav2vec2_bert.encoder.layers[23].ffn2.parameters():\n                param.requires_grad = True\n\n            # Turn back on final layer norm\n            for param in embedding.model.wav2vec2_bert.encoder.layers[23].final_layer_norm.parameters():\n                param.requires_grad = True\n                \n            # Turn back on projector\n            for param in embedding.model.projector.parameters():\n                param.requires_grad = True\n                \n            # Turn back on classifier\n            for param in embedding.model.classifier.parameters():\n                param.requires_grad = True\n    except NameError:\n         None\n\n    try:\n        if isinstance(embedding, HubertEmbedding):\n            # Freeze all the layers\n            for param in embedding.model.parameters():\n                param.requires_grad = False\n\n            # Turn back on last feed forward layer\n            for param in embedding.model.hubert.encoder.layers[11].feed_forward.parameters():\n                param.requires_grad = True\n\n            # Turn back on final layer norm\n            for param in embedding.model.hubert.encoder.layers[11].final_layer_norm.parameters():\n                param.requires_grad = True\n                \n            # Turn back on projector\n            for param in embedding.model.projector.parameters():\n                param.requires_grad = True\n                \n            # Turn back on classifier\n            for param in embedding.model.classifier.parameters():\n                param.requires_grad = True\n    except NameError:\n        None","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:30.714582Z","iopub.execute_input":"2024-08-24T17:56:30.714954Z","iopub.status.idle":"2024-08-24T17:56:30.729435Z","shell.execute_reply.started":"2024-08-24T17:56:30.714925Z","shell.execute_reply":"2024-08-24T17:56:30.728407Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Choose a model from one of the classes defined above\n\n#model = W2VBertModelEmbedding()\n#model = W2VBSeqEmbedding()\n#model = HubertEmbedding()\nmodel = ResNetEmbedding()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:56:34.281083Z","iopub.execute_input":"2024-08-24T17:56:34.282175Z","iopub.status.idle":"2024-08-24T17:56:35.131582Z","shell.execute_reply.started":"2024-08-24T17:56:34.282136Z","shell.execute_reply":"2024-08-24T17:56:35.130596Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 134MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader, val_loader = make_train_loaders(model, \n                                              '/kaggle/input/augmented-music/batch_1_augmented_16000Hz.pkl', \n                                              train_batch_size = 16)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:57:38.777963Z","iopub.execute_input":"2024-08-24T17:57:38.778358Z","iopub.status.idle":"2024-08-24T17:57:40.100479Z","shell.execute_reply.started":"2024-08-24T17:57:38.778329Z","shell.execute_reply":"2024-08-24T17:57:40.099725Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print('This model has', str(sum(p.numel() for p in model.parameters() if p.requires_grad)), 'trainable parameters.')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:57:15.172943Z","iopub.execute_input":"2024-08-24T17:57:15.173626Z","iopub.status.idle":"2024-08-24T17:57:15.179398Z","shell.execute_reply.started":"2024-08-24T17:57:15.173593Z","shell.execute_reply":"2024-08-24T17:57:15.178320Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"This model has 11235904 trainable parameters.\n","output_type":"stream"}]},{"cell_type":"code","source":"setup_model(model)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:57:22.049012Z","iopub.execute_input":"2024-08-24T17:57:22.049924Z","iopub.status.idle":"2024-08-24T17:57:22.054967Z","shell.execute_reply.started":"2024-08-24T17:57:22.049880Z","shell.execute_reply":"2024-08-24T17:57:22.053793Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print('This model now has', str(sum(p.numel() for p in model.parameters() if p.requires_grad)), 'trainable parameters.')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:57:23.756771Z","iopub.execute_input":"2024-08-24T17:57:23.757616Z","iopub.status.idle":"2024-08-24T17:57:23.763387Z","shell.execute_reply.started":"2024-08-24T17:57:23.757583Z","shell.execute_reply":"2024-08-24T17:57:23.762414Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"This model now has 8459392 trainable parameters.\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\ntrain_losses = []\nval_losses = []\nbaseline_losses = []\n\nnum_epochs = 1","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:57:59.285624Z","iopub.execute_input":"2024-08-24T17:57:59.286472Z","iopub.status.idle":"2024-08-24T17:57:59.292669Z","shell.execute_reply.started":"2024-08-24T17:57:59.286438Z","shell.execute_reply":"2024-08-24T17:57:59.291740Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T17:58:00.430751Z","iopub.execute_input":"2024-08-24T17:58:00.431519Z","iopub.status.idle":"2024-08-24T17:58:00.730227Z","shell.execute_reply.started":"2024-08-24T17:58:00.431491Z","shell.execute_reply":"2024-08-24T17:58:00.729298Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"ResNetEmbedding(\n  (resnet): ResNet(\n    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=512, out_features=128, bias=True)\n  )\n  (dropout): Dropout(p=0.8, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"\nscaler = torch.cuda.amp.GradScaler()\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    running_train_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\", unit=\"sample\", position=0, leave=True)\n    #train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n    # Loop over batches using dataloaders\n    for anchors, positives, negatives in train_loader:\n        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n\n        optimizer.zero_grad()\n            \n        with torch.autocast(device.type):\n            try:\n                if isinstance(model, (W2VBertModelEmbedding,W2VBSeqEmbedding)):\n                    anchor_embeddings = model(anchors.squeeze())\n                    positive_embeddings = model(positives.squeeze())\n                    negative_embeddings = model(negatives.squeeze())\n                elif isinstance(model, HubertEmbedding):\n                    anchor_embeddings = model(anchors)\n                    positive_embeddings = model(positives)\n                    negative_embeddings = model(negatives)\n                elif isinstance(model, ResNetEmbedding):\n                    anchor_embeddings = model(anchors)\n                    positive_embeddings = model(positives)\n                    negative_embeddings = model(negatives)\n            except NameError:\n                None\n\n            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n    \n        del anchor_embeddings, positive_embeddings, negative_embeddings\n    \n        # Scales the loss, and calls backward() to create scaled gradients\n        scaler.scale(loss).backward()\n            \n        # Unscales gradients and calls or skips optimizer.step()\n        scaler.step(optimizer)\n        \n        # Updates the scale for next iteration\n        scaler.update()\n        #loss.backward()\n        #optimizer.step()\n        \n        try:\n            if isinstance(model, (W2VBertModelEmbedding,W2VBSeqEmbedding, ResNetEmbedding)):\n                running_train_loss += loss.item() * anchors.size(0)\n            elif isinstance(model, HubertEmbedding):\n                running_train_loss += loss.item() * anchors.input_values.size(0)\n        except NameError:\n            None\n        # Update the progress bar by the current batch size\n        pbar.set_postfix({'loss': loss.item()})\n        pbar.update(1)  # Increment the progress bar\n        #del loss\n          \n    train_loss = running_train_loss / len(train_loader.dataset)\n    train_losses.append(train_loss)\n\n    # Turn on validation/eval mode\n    model.eval()\n    running_val_loss = 0.0 \n    running_baseline_loss = 0.0   \n    val_pbar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\", position=0, leave=True)\n    # Turn off gradient updates since we're in validation\n    with torch.no_grad():\n        # Batch loop \n        for anchors, positives, negatives in val_loader:\n            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n            \n            try:\n                if isinstance(model, (W2VBertModelEmbedding,W2VBSeqEmbedding)):\n                    anchor_embeddings = model(anchors.squeeze())\n                    positive_embeddings = model(positives.squeeze())\n                    negative_embeddings = model(negatives.squeeze())\n                elif isinstance(model, HubertEmbedding):\n                    anchor_embeddings = model(anchors)\n                    positive_embeddings = model(positives)\n                    negative_embeddings = model(negatives)\n                elif isinstance(model, ResNetEmbedding):\n                    anchor_embeddings = model(anchors)\n                    positive_embeddings = model(positives)\n                    negative_embeddings = model(negatives)\n            except NameError:\n                None\n            \n            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n\n            # Add to running val loss\n            running_val_loss += loss.item() * anchors.size(0)\n            \n            # baseline loss\n            baseline_loss = criterion(F.normalize(anchors), \n                                               F.normalize(positives), \n                                               F.normalize(negatives)).item()\n            running_baseline_loss += baseline_loss*anchors.size(0)\n            \n            # Update the validation progress bar\n            val_pbar.update(1)  # Increment the progress bar\n            #val_pbar.update(anchors.size(0))\n            #val_pbar.set_postfix(loss=loss.item())\n    \n    # Calculate average validation loss over the entire dataset\n    val_loss = running_val_loss / len(val_loader.dataset)\n    val_losses.append(val_loss)\n    # Do the same for the baseline\n    baseline_avg_loss = running_baseline_loss / len(val_loader.dataset)\n    baseline_losses.append(baseline_avg_loss)\n\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Baseline Loss: {baseline_avg_loss:.4f}\")\n    \n    with open('training_logs.pkl', 'wb') as f:\n        pkl.dump((train_losses, val_losses, baseline_losses), f)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T18:10:44.110886Z","iopub.execute_input":"2024-08-24T18:10:44.111412Z","iopub.status.idle":"2024-08-24T18:12:03.355465Z","shell.execute_reply.started":"2024-08-24T18:10:44.111381Z","shell.execute_reply":"2024-08-24T18:12:03.354292Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Training 1/1: 100%|██████████| 50/50 [04:05<00:00,  4.91s/sample, loss=0.625]\nValidation 1/1: 100%|██████████| 13/13 [04:07<00:00, 19.01s/batch]loss=0.378]\nValidation 1/1: 100%|██████████| 13/13 [00:16<00:00,  1.05s/batch]","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Train Loss: 0.5380, Val Loss: 0.7557, Baseline Loss: 0.9980\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    out1 = model(train_dataset[0][0].to(device))\n    out2 = model(train_dataset[0][1].to(device))\n    out3 = model(train_dataset[0][2].to(device))","metadata":{"execution":{"iopub.execute_input":"2024-08-22T15:07:43.213228Z","iopub.status.busy":"2024-08-22T15:07:43.212844Z","iopub.status.idle":"2024-08-22T15:07:43.694548Z","shell.execute_reply":"2024-08-22T15:07:43.693265Z","shell.execute_reply.started":"2024-08-22T15:07:43.213197Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"torch.nn.functional.cosine_similarity(out1,out2)","metadata":{"execution":{"iopub.execute_input":"2024-08-22T15:07:56.821072Z","iopub.status.busy":"2024-08-22T15:07:56.820391Z","iopub.status.idle":"2024-08-22T15:07:56.830696Z","shell.execute_reply":"2024-08-22T15:07:56.829828Z","shell.execute_reply.started":"2024-08-22T15:07:56.821020Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":["tensor([0.9950], device='cuda:0')"]},"metadata":{}}]},{"cell_type":"code","source":"torch.nn.functional.cosine_similarity(out1,out3)","metadata":{"execution":{"iopub.execute_input":"2024-08-22T15:07:59.109160Z","iopub.status.busy":"2024-08-22T15:07:59.108359Z","iopub.status.idle":"2024-08-22T15:07:59.116124Z","shell.execute_reply":"2024-08-22T15:07:59.115344Z","shell.execute_reply.started":"2024-08-22T15:07:59.109127Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":["tensor([0.9946], device='cuda:0')"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}