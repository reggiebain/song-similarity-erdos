{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Finetuning Transformer embeddings\n","- Create Siamese network with transformer archetecture\n","- Finetune pretrained models to determine song similarity\n","- The pretrained models considered are\n","    - Wav2VecBertModel from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/wav2vec2-bert\n","    - Wav2Vec2BertForSequenceClassification model from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification\n","    - HubertForSequenceClassification model from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/hubert#transformers.HubertForSequenceClassification\n","    - ASTModel from https://huggingface.co/docs/transformers/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:03.530566Z","iopub.status.busy":"2024-08-27T19:17:03.529795Z","iopub.status.idle":"2024-08-27T19:17:33.278838Z","shell.execute_reply":"2024-08-27T19:17:33.277923Z","shell.execute_reply.started":"2024-08-27T19:17:03.530530Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-08-27 19:17:18.633850: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-27 19:17:18.633970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-27 19:17:18.940433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import librosa\n","import librosa.display\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","#from dotenv import dotenv_values \n","#import spotipy\n","#from spotipy.oauth2 import SpotifyClientCredentials\n","import pickle as pkl\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset\n","\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import  HubertForSequenceClassification, Wav2Vec2BertModel, Wav2Vec2BertForSequenceClassification, AutoProcessor, AutoFeatureExtractor, ASTModel"]},{"cell_type":"markdown","metadata":{},"source":["## Define Dataset classes for each model"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.280873Z","iopub.status.busy":"2024-08-27T19:17:33.280312Z","iopub.status.idle":"2024-08-27T19:17:33.289428Z","shell.execute_reply":"2024-08-27T19:17:33.288483Z","shell.execute_reply.started":"2024-08-27T19:17:33.280846Z"},"trusted":true},"outputs":[],"source":["# Dataset class for the Wav2Vec2BertModel embedding\n","class W2VBertModelDataset(Dataset):\n","\n","    def __init__(self, audio, sr=16000):\n","\n","        self.audio = audio\n","        self.sr = sr\n","        self.feature_extractor = AutoProcessor.from_pretrained(\"hf-audio/wav2vec2-bert-CV16-en\")\n","    \n","    def __len__(self):\n","        return self.audio.shape[0]\n","    \n","    def __getitem__(self, idx):\n","\n","        # Load audio data and select idx'th example and get [0] to get audio from (y, sr) tuple\n","        anchor = self.audio['processed_audio'].values[idx][0]#.astype(np.float16)\n","        positive = self.audio['augmented_audio'].values[idx][0]#.astype(np.float16)\n","        negative = self.audio['diff_processed_audio'].values[idx][0]#.astype(np.float16)\n","\n","        # Preprocess data\n","        anchor_mel = self.feature_extractor(anchor, sampling_rate=self.sr, return_tensors=\"pt\").input_features\n","        positive_mel = self.feature_extractor(positive, sampling_rate=self.sr, return_tensors=\"pt\").input_features\n","        negative_mel = self.feature_extractor(negative, sampling_rate=self.sr, return_tensors=\"pt\").input_features\n","\n","        return anchor_mel, positive_mel, negative_mel"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.290827Z","iopub.status.busy":"2024-08-27T19:17:33.290548Z","iopub.status.idle":"2024-08-27T19:17:33.326925Z","shell.execute_reply":"2024-08-27T19:17:33.326173Z","shell.execute_reply.started":"2024-08-27T19:17:33.290803Z"},"trusted":true},"outputs":[],"source":["# Dataset class for the Wav2Vec2BertSequence model\n","class W2VBSeqDataset(Dataset):\n","\n","    def __init__(self, audio, sr=16000):\n","\n","        self.audio = audio\n","        self.sr = sr\n","        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n","    \n","    def __len__(self):\n","        return self.audio.shape[0]\n","    \n","    def __getitem__(self, idx):\n","\n","        # Load audio data and select idx'th example and get [0] to get audio from (y, sr) tuple\n","        anchor = self.audio['processed_audio'].values[idx][0]#.astype(np.float16)\n","        positive = self.audio['augmented_audio'].values[idx][0]#.astype(np.float16)\n","        negative = self.audio['diff_processed_audio'].values[idx][0]#.astype(np.float16)\n","\n","        # Preprocess data\n","        anchor_mel = self.feature_extractor(anchor, sampling_rate=self.sr, return_tensors=\"pt\").input_features\n","        positive_mel = self.feature_extractor(positive, sampling_rate=self.sr, return_tensors=\"pt\").input_features\n","        negative_mel = self.feature_extractor(negative, sampling_rate=self.sr, return_tensors=\"pt\").input_features\n","\n","        return anchor_mel, positive_mel, negative_mel"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.329686Z","iopub.status.busy":"2024-08-27T19:17:33.329354Z","iopub.status.idle":"2024-08-27T19:17:33.344682Z","shell.execute_reply":"2024-08-27T19:17:33.343977Z","shell.execute_reply.started":"2024-08-27T19:17:33.329663Z"},"trusted":true},"outputs":[],"source":["# Dataset class for the Hubert model\n","class HubertDataset(Dataset):\n","\n","    def __init__(self, audio):\n","\n","        self.audio = audio\n","        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"superb/hubert-base-superb-ks\")\n","    \n","    def __len__(self):\n","        return self.audio.shape[0]\n","    \n","    def __getitem__(self, idx):\n","\n","        # Load audio data and select idx'th example and get [0] to get audio from (y, sr) tuple\n","        anchor = self.audio['processed_audio'].values[idx][0]#.astype(np.float16)\n","        positive = self.audio['augmented_audio'].values[idx][0]#.astype(np.float16)\n","        negative = self.audio['diff_processed_audio'].values[idx][0]#.astype(np.float16)\n","    \n","        anchor = self.feature_extractor(anchor, sampling_rate=16000, return_tensors=\"pt\").input_values\n","        positive = self.feature_extractor(positive, sampling_rate=16000, return_tensors=\"pt\").input_values\n","        negative = self.feature_extractor(negative, sampling_rate=16000, return_tensors=\"pt\").input_values\n","\n","        return anchor, positive, negative\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.345999Z","iopub.status.busy":"2024-08-27T19:17:33.345695Z","iopub.status.idle":"2024-08-27T19:17:33.362934Z","shell.execute_reply":"2024-08-27T19:17:33.362224Z","shell.execute_reply.started":"2024-08-27T19:17:33.345975Z"},"trusted":true},"outputs":[],"source":["# Dataset class for the Wav2Vec2BertModel embedding\n","class ASTModelDataset(Dataset):\n","\n","    def __init__(self, audio, sr=16000):\n","\n","        self.audio = audio\n","        self.sr = sr\n","        self.feature_extractor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n","    \n","    def __len__(self):\n","        return self.audio.shape[0]\n","    \n","    def __getitem__(self, idx):\n","\n","        # Load audio data and select idx'th example and get [0] to get audio from (y, sr) tuple\n","        anchor = self.audio['processed_audio'].values[idx][0]#.astype(np.float16)\n","        positive = self.audio['augmented_audio'].values[idx][0]#.astype(np.float16)\n","        negative = self.audio['diff_processed_audio'].values[idx][0]#.astype(np.float16)\n","\n","        # Compute mel spectrograms\n","        anchor_mel = self.feature_extractor(anchor, sampling_rate=self.sr, return_tensors=\"pt\").input_values\n","        positive_mel = self.feature_extractor(positive, sampling_rate=self.sr, return_tensors=\"pt\").input_values\n","        negative_mel = self.feature_extractor(negative, sampling_rate=self.sr, return_tensors=\"pt\").input_values\n","\n","        return anchor_mel, positive_mel, negative_mel"]},{"cell_type":"markdown","metadata":{},"source":["## Define model classes"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.384566Z","iopub.status.busy":"2024-08-27T19:17:33.383936Z","iopub.status.idle":"2024-08-27T19:17:33.402754Z","shell.execute_reply":"2024-08-27T19:17:33.401991Z","shell.execute_reply.started":"2024-08-27T19:17:33.384518Z"},"trusted":true},"outputs":[],"source":["# Use the Wav2VecBertModel from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/wav2vec2-bert\n","# with an added fully connected layer to create embedding for Siamese network\n","\n","class W2VBertModelEmbedding(nn.Module):\n","    def __init__(self, input_len = 160, embedding_dim=128):\n","        super(W2VBertModelEmbedding, self).__init__()\n","\n","        # Load pre-trained Wav2Vec model\n","        self.model = Wav2Vec2BertModel.from_pretrained(\"hf-audio/wav2vec2-bert-CV16-en\")\n","        self.model.feature_projection.layer_norm = nn.LayerNorm((input_len,), eps=1e-05, elementwise_affine=True)\n","        self.model.feature_projection.projection = nn.Linear(in_features=input_len, out_features=1024, bias=True)\n","        \n","        # Add a fully connected layer to project the hidden states to the desired embedding dimension\n","        self.fc = nn.Linear(self.model.config.hidden_size, embedding_dim)\n","        \n","        #self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        # Extract the last hidden state from the model\n","        x = self.model(x).last_hidden_state\n","        \n","        x = self.fc(x[:,0,:])\n","        \n","        #x = self.relu(x)\n","        \n","        # Normalize the output embeddings\n","        return F.normalize(x, p=2, dim=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.404734Z","iopub.status.busy":"2024-08-27T19:17:33.403865Z","iopub.status.idle":"2024-08-27T19:17:33.420209Z","shell.execute_reply":"2024-08-27T19:17:33.419500Z","shell.execute_reply.started":"2024-08-27T19:17:33.404706Z"},"trusted":true},"outputs":[],"source":["# Use the Wav2Vec2BertForSequenceClassification model from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification\n","# to create embedding for Siamese network\n","class W2VBSeqEmbedding(nn.Module):\n","    def __init__(self, input_len = 160, embedding_dim=128):\n","        super(W2VBSeqEmbedding, self).__init__()\n","\n","        # Load pre-trained Wav2Vec model\n","        self.model = Wav2Vec2BertForSequenceClassification.from_pretrained(\"facebook/w2v-bert-2.0\")\n","        self.model.wav2vec2_bert.feature_projection.layer_norm = nn.LayerNorm((input_len,), eps=1e-05, elementwise_affine=True)\n","        self.model.wav2vec2_bert.feature_projection.projection = nn.Linear(in_features=input_len, out_features=1024, bias=True)\n","        \n","        # Add a fully connected layer to project the hidden states to the desired embedding dimension\n","        self.model.classifier = nn.Linear(self.model.classifier.in_features, embedding_dim)\n","        \n","    def forward(self, x):\n","        \n","        x = self.model(x).logits\n","        \n","        # Normalize the output embeddings\n","        return F.normalize(x, p=2, dim=1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.421476Z","iopub.status.busy":"2024-08-27T19:17:33.421205Z","iopub.status.idle":"2024-08-27T19:17:33.440422Z","shell.execute_reply":"2024-08-27T19:17:33.439753Z","shell.execute_reply.started":"2024-08-27T19:17:33.421430Z"},"trusted":true},"outputs":[],"source":["# Use the HubertForSequenceClassification model from https://huggingface.co/docs/transformers/v4.44.1/en/model_doc/hubert#transformers.HubertForSequenceClassification\n","# to create embedding for Siamese network\n","\n","class HubertEmbedding(nn.Module):\n","    def __init__(self, embedding_dim=128):\n","        super(HubertEmbedding, self).__init__()\n","        \n","        # Load pre-trained Hubert model\n","        self.model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-base-superb-ks\")\n","        \n","        # Add a fully connected layer to project the hidden states to the desired embedding dimension\n","        self.model.classifier = nn.Linear(self.model.classifier.in_features, embedding_dim)\n","\n","    def forward(self, x):\n","        # Extract the last hidden state from the DistilHuBERT model\n","        x = self.model(x.squeeze(1)).logits#.last_hidden_state\n","        \n","        # Apply the fully connected layer to reduce the dimension\n","        #x = self.fc(x.mean(dim=1))  # Take mean over the time dimension\n","        \n","        # Normalize the output embeddings\n","        return F.normalize(x, p=2, dim=1)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.444293Z","iopub.status.busy":"2024-08-27T19:17:33.443943Z","iopub.status.idle":"2024-08-27T19:17:33.457709Z","shell.execute_reply":"2024-08-27T19:17:33.456739Z","shell.execute_reply.started":"2024-08-27T19:17:33.444258Z"},"trusted":true},"outputs":[],"source":["class AudioSpecTransformerModel(torch.nn.Module):\n","    def __init__(self, embedding_dim=128):\n","        super(AudioSpecTransformerModel, self).__init__()\n","        \n","        self.model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n","\n","        self.fc = torch.nn.Linear(self.model.config.hidden_size, embedding_dim)\n","\n","    def forward(self, x):\n","        outputs = self.model(x).last_hidden_state\n","        \n","        # Get [cls] token embedding for classification/summary of embedding\n","        x = self.fc(outputs[:, 0, :])\n","        \n","        return F.normalize(x, p=2, dim=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Make helper functions"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.471285Z","iopub.status.busy":"2024-08-27T19:17:33.470757Z","iopub.status.idle":"2024-08-27T19:17:33.489325Z","shell.execute_reply":"2024-08-27T19:17:33.488564Z","shell.execute_reply.started":"2024-08-27T19:17:33.471254Z"},"trusted":true},"outputs":[],"source":["\n","def make_train_loaders(model, dir_path = '/kaggle/input/augmented-music/combined_batch_augmented.pkl', dataset_size = 10000, train_batch_size = 16, val_batch_size = 16):\n","\n","    '''\n","        Creates DataLoaders to be used in training and validation\n","\n","        Arguments: \n","            model -- model to be trained\n","            dataset_size -- int, size of dataset to be imported\n","            dir_path -- path to dataset\n","            train(val)_batch_size -- size of training (validation) batch sizes\n","        Returns:\n","            train(val)_loader -- DataLoader of training (validation) samples\n","    '''\n","    \n","\n","    audio_data = pd.read_pickle(dir_path).iloc[:dataset_size]\n","    \n","    # Make sure the requested dataset size is no bigger than the given dataset\n","    assert dataset_size <= len(audio_data), 'dataset_size is larger than the imported dataset'\n","    \n","    # Make sure the dataset has 'processed_audio', 'augmented_audio', and 'diff_processed_audio' columns\n","    assert 'processed_audio' and 'augmented_audio' and 'diff_processed_audio' in audio_data.columns, 'DataFrame must contain columns \\'processed_audio\\', \\'augmented_audio\\', and \\'diff_processed_audio\\''\n","\n","    # Split the data into training and validation sets\n","    train_data, val_data = train_test_split(audio_data, test_size=0.2, random_state=123)\n","\n","    try:\n","        if isinstance(model, W2VBSeqEmbedding):\n","            train_dataset = W2VBSeqDataset(train_data)\n","            val_dataset = W2VBSeqDataset(val_data)\n","        elif isinstance(model, W2VBertModelEmbedding):\n","            train_dataset = W2VBertModelDataset(train_data)\n","            val_dataset = W2VBertModelDataset(val_data)            \n","        elif isinstance(model, HubertEmbedding):\n","            train_dataset = HubertDataset(train_data)\n","            val_dataset = HubertDataset(val_data)\n","        elif isinstance(model, AudioSpecTransformerModel):\n","            train_dataset = ASTModelDataset(train_data)\n","            val_dataset = ASTModelDataset(val_data)\n","    except NameError:\n","        None\n","\n","\n","    # Define dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n","\n","    return train_loader, val_loader"]},{"cell_type":"markdown","metadata":{},"source":["## Set up the model by freezing all but the deepest layers"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.491029Z","iopub.status.busy":"2024-08-27T19:17:33.490761Z","iopub.status.idle":"2024-08-27T19:17:33.509329Z","shell.execute_reply":"2024-08-27T19:17:33.508477Z","shell.execute_reply.started":"2024-08-27T19:17:33.491006Z"},"trusted":true},"outputs":[],"source":["#Freeze layers for finetuning\n","def setup_model(embedding):\n","    '''\n","    Freeze all but the last few layers of the model.\n","    Argument:\n","        embedding -- model, must be one of the embedding types defined above\n","    Returns:\n","        None\n","    '''\n","\n","    try:\n","        if isinstance(embedding, W2VBertModelEmbedding):\n","            # Freeze all the layers\n","            for param in embedding.model.parameters():\n","                param.requires_grad = False\n","\n","            # Turn back on adapter\n","            for param in embedding.model.adapter.parameters():\n","                param.requires_grad = True\n","                \n","            # Turn back on last fully connected layer\n","            for param in embedding.fc.parameters():\n","                param.requires_grad = True\n","    except NameError:\n","        None\n","\n","    try:\n","        if isinstance(embedding, W2VBSeqEmbedding):\n","\n","            # Freeze all the layers\n","            for param in embedding.model.parameters():\n","                param.requires_grad = False\n","\n","            for i in range(21,23):\n","                # Turn back on ith encoder layer\n","                for param in embedding.model.wav2vec2_bert.encoder.layers[i].parameters():\n","                    param.requires_grad = True\n","                \n","            # Turn back on projector\n","            for param in embedding.model.projector.parameters():\n","                param.requires_grad = True\n","                \n","            # Turn back on classifier\n","            for param in embedding.model.classifier.parameters():\n","                param.requires_grad = True\n","    except NameError:\n","         None\n","\n","    try:\n","        if isinstance(embedding, HubertEmbedding):\n","            # Freeze all the layers\n","            for param in embedding.model.parameters():\n","                param.requires_grad = False\n","\n","            for param in embedding.model.hubert.encoder.pos_conv_embed.conv.parametrizations.weight:\n","                param.requires_grad = False\n","                \n","            # Turn back on last layer\n","            for param in embedding.model.hubert.encoder.layers[11].parameters():\n","                param.requires_grad = True\n","                \n","            # Turn back on projector\n","            for param in embedding.model.projector.parameters():\n","                param.requires_grad = True\n","                \n","            # Turn back on classifier\n","            for param in embedding.model.classifier.parameters():\n","                param.requires_grad = True\n","    except NameError:\n","        None\n","        \n","    try:\n","        if isinstance(embedding, AudioSpecTransformerModel):\n","            # Freeze all the layers\n","            for param in embedding.model.parameters():\n","                param.requires_grad = False\n","\n","            for i in range(7,11):\n","                # Turn back on ith encoder layer\n","                for param in embedding.model.encoder.layer[i].parameters():\n","                    param.requires_grad = True\n","                \n","            # Turn back on projector\n","            for param in embedding.model.layernorm.parameters():\n","                param.requires_grad = True\n","                \n","            # Turn back on last fully connected layer\n","            for param in embedding.fc.parameters():\n","                param.requires_grad = True\n","    except NameError:\n","        None\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["## Choose a model, define DataLoaders, choose optimizer, etc"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:33.510775Z","iopub.status.busy":"2024-08-27T19:17:33.510499Z","iopub.status.idle":"2024-08-27T19:17:45.176171Z","shell.execute_reply":"2024-08-27T19:17:45.175354Z","shell.execute_reply.started":"2024-08-27T19:17:33.510752Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb9aa00939984071a4bf136c74745199","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90d190e21f6a423b806b0642b8879ad2","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Choose a model from one of the classes defined above\n","\n","model = AudioSpecTransformerModel()\n","#model = W2VBertModelEmbedding()\n","#model = W2VBSeqEmbedding()\n","#model = HubertEmbedding()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:45.177482Z","iopub.status.busy":"2024-08-27T19:17:45.177192Z","iopub.status.idle":"2024-08-27T19:17:58.028569Z","shell.execute_reply":"2024-08-27T19:17:58.027622Z","shell.execute_reply.started":"2024-08-27T19:17:45.177456Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35076c9020844d81bc2dc4e9491df7b6","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/275 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_loader, val_loader = make_train_loaders(model, \n","                                              '/kaggle/input/augmented-music/batch_1_augmented_16000Hz.pkl',\n","                                              dataset_size = 1000,\n","                                              train_batch_size = 16,\n","                                              val_batch_size = 16)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:58.030561Z","iopub.status.busy":"2024-08-27T19:17:58.029885Z","iopub.status.idle":"2024-08-27T19:17:58.048830Z","shell.execute_reply":"2024-08-27T19:17:58.047884Z","shell.execute_reply.started":"2024-08-27T19:17:58.030525Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Before set up, this model has 581378752 trainable parameters.\n","After set up, this model now has 49246208 trainable parameters.\n"]}],"source":["print('Before set up, this model has', str(sum(p.numel() for p in model.parameters() if p.requires_grad)), 'trainable parameters.')\n","setup_model(model)\n","print('After set up, this model now has', str(sum(p.numel() for p in model.parameters() if p.requires_grad)), 'trainable parameters.')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:58.050237Z","iopub.status.busy":"2024-08-27T19:17:58.049985Z","iopub.status.idle":"2024-08-27T19:17:58.064887Z","shell.execute_reply":"2024-08-27T19:17:58.064110Z","shell.execute_reply.started":"2024-08-27T19:17:58.050215Z"},"trusted":true},"outputs":[],"source":["criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n","optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n","train_losses = []\n","val_losses = []\n","baseline_losses = []\n","\n","num_epochs = 3"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:58.066186Z","iopub.status.busy":"2024-08-27T19:17:58.065937Z","iopub.status.idle":"2024-08-27T19:17:58.941974Z","shell.execute_reply":"2024-08-27T19:17:58.940862Z","shell.execute_reply.started":"2024-08-27T19:17:58.066164Z"},"trusted":true},"outputs":[{"data":{"text/plain":["W2VBSeqEmbedding(\n","  (model): Wav2Vec2BertForSequenceClassification(\n","    (wav2vec2_bert): Wav2Vec2BertModel(\n","      (feature_projection): Wav2Vec2BertFeatureProjection(\n","        (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (projection): Linear(in_features=160, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): Wav2Vec2BertEncoder(\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-23): 24 x Wav2Vec2BertEncoderLayer(\n","            (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (ffn1): Wav2Vec2BertFeedForward(\n","              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n","              (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n","              (intermediate_act_fn): SiLU()\n","              (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n","              (output_dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (self_attn_dropout): Dropout(p=0.0, inplace=False)\n","            (self_attn): Wav2Vec2BertSelfAttention(\n","              (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n","              (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n","              (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n","              (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (distance_embedding): Embedding(73, 64)\n","            )\n","            (conv_module): Wav2Vec2BertConvolutionModule(\n","              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n","              (glu): GLU(dim=1)\n","              (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), groups=1024, bias=False)\n","              (depthwise_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (activation): SiLU()\n","              (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (ffn2): Wav2Vec2BertFeedForward(\n","              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n","              (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n","              (intermediate_act_fn): SiLU()\n","              (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n","              (output_dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (projector): Linear(in_features=1024, out_features=768, bias=True)\n","    (classifier): Linear(in_features=768, out_features=128, bias=True)\n","  )\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T19:17:58.944136Z","iopub.status.busy":"2024-08-27T19:17:58.943512Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Validation 1/3: 100%|██████████| 13/13 [02:43<00:00, 11.03s/batch]loss=0.436]"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/3], Train Loss: 0.9927, Val Loss: 0.9815, Baseline Loss: 0.7368\n"]},{"name":"stderr","output_type":"stream","text":["Training 1/3: 100%|██████████| 50/50 [08:29<00:00, 10.19s/sample, loss=0.436]\n","Validation 1/3: 100%|██████████| 13/13 [08:35<00:00, 39.66s/batch]loss=1.13] \n","Validation 2/3:  23%|██▎       | 3/13 [00:38<02:08, 12.83s/batch]"]}],"source":["\n","scaler = torch.cuda.amp.GradScaler()\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    running_train_loss = 0.0\n","    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\", unit=\"sample\", position=0, leave=True)\n","    #train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n","    # Loop over batches using dataloaders\n","    for anchors, positives, negatives in train_loader:\n","        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n","\n","        optimizer.zero_grad()\n","            \n","        with torch.autocast(device.type):\n","            try:\n","                if isinstance(model, (W2VBertModelEmbedding, W2VBSeqEmbedding, AudioSpecTransformerModel)):\n","                    anchor_embeddings = model(anchors.squeeze())\n","                    positive_embeddings = model(positives.squeeze())\n","                    negative_embeddings = model(negatives.squeeze())\n","                else:\n","                    anchor_embeddings = model(anchors)\n","                    positive_embeddings = model(positives)\n","                    negative_embeddings = model(negatives)\n","            except NameError:\n","                None\n","\n","            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n","    \n","        del anchor_embeddings, positive_embeddings, negative_embeddings\n","    \n","        # Scales the loss, and calls backward() to create scaled gradients\n","        scaler.scale(loss).backward()\n","            \n","        # Unscales gradients and calls or skips optimizer.step()\n","        scaler.step(optimizer)\n","        \n","        # Updates the scale for next iteration\n","        scaler.update()\n","        #loss.backward()\n","        #optimizer.step()\n","        \n","        running_train_loss += loss.item() * anchors.size(0)\n","        \n","        # Update the progress bar by the current batch size\n","        pbar.set_postfix({'loss': loss.item()})\n","        pbar.update(1)  # Increment the progress bar\n","        #del loss\n","          \n","    train_loss = running_train_loss / len(train_loader.dataset)\n","    train_losses.append(train_loss)\n","\n","    # Turn on validation/eval mode\n","    model.eval()\n","    running_val_loss = 0.0 \n","    running_baseline_loss = 0.0   \n","    val_pbar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\", position=0, leave=True)\n","    # Turn off gradient updates since we're in validation\n","    with torch.no_grad():\n","        # Batch loop \n","        for anchors, positives, negatives in val_loader:\n","            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n","            \n","            try:\n","                if isinstance(model, (W2VBertModelEmbedding, W2VBSeqEmbedding, AudioSpecTransformerModel)):\n","                    anchor_embeddings = model(anchors.squeeze())\n","                    positive_embeddings = model(positives.squeeze())\n","                    negative_embeddings = model(negatives.squeeze())\n","                else:\n","                    anchor_embeddings = model(anchors)\n","                    positive_embeddings = model(positives)\n","                    negative_embeddings = model(negatives)\n","            except NameError:\n","                None\n","            \n","            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n","\n","            # Add to running val loss\n","            running_val_loss += loss.item() * anchors.size(0)\n","            \n","            # baseline loss\n","            baseline_loss = criterion(F.normalize(anchors), \n","                                               F.normalize(positives), \n","                                               F.normalize(negatives)).item()\n","            running_baseline_loss += baseline_loss*anchors.size(0)\n","            \n","            # Update the validation progress bar\n","            val_pbar.update(1)  # Increment the progress bar\n","            #val_pbar.update(anchors.size(0))\n","            #val_pbar.set_postfix(loss=loss.item())\n","    \n","    # Calculate average validation loss over the entire dataset\n","    val_loss = running_val_loss / len(val_loader.dataset)\n","    val_losses.append(val_loss)\n","    # Do the same for the baseline\n","    baseline_avg_loss = running_baseline_loss / len(val_loader.dataset)\n","    baseline_losses.append(baseline_avg_loss)\n","\n","    \n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Baseline Loss: {baseline_avg_loss:.4f}\")\n","    \n","    with open('training_logs.pkl', 'wb') as f:\n","        pkl.dump((train_losses, val_losses, baseline_losses), f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot loss curves for training\n","epochs = range(1, num_epochs + 1)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","plt.plot(epochs, baseline_losses, label='Baseline Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('audio-spec-loss-plot.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.eval()\n","\n","a,p,n = next(iter(train_loader))\n","\n","with torch.no_grad():\n","    out1 = model(a[0].to(device))\n","    out2 = model(p[0].to(device))\n","    out3 = model(n[0].to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.nn.functional.cosine_similarity(out1,out2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.nn.functional.cosine_similarity(out1,out3)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5435569,"sourceId":9020098,"sourceType":"datasetVersion"},{"datasetId":5539947,"sourceId":9219897,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
