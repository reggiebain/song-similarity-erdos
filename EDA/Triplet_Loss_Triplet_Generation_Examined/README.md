
During this project, we ran into the issue of creating "well-defined" triples of songs for implementation of triplet loss in relation to training a siamese network. There are only so many covers of a song or examples of a song being sampled. Similarly, creating triplets that consist only of audio augmentations introduces a clear bias to the network as it would likely equate "similar" and "augmented." Additionally searching for, aquiring, and augmentating audio data proved to be an exhausting task that would have made a decently sized project in its own right. The notebooks entitled "MusicInfoAnalysisandMr.Brightside.ipynb", "K-ClusteringMusicInfo3Clusters.ipynb", "IterativeLearning_3clusters.ipynb," and "BatchTraining_SGD_3clusters_savedmodels.ipynb" use K-Means, iterative learning, and stochastic gradient descent (SGD) to attempt to improve the triplet collection process. 

The audio we ended up concentrating on in this project was pulled from a csv file called "music_info.csv." It is a collection of metadata of over 50,000 songs. Among the data labels are: 

1. danceability - Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.
2. energy - Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
3. key - The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.
4. mode - Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.
5. tempo - The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
6. valence - A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

Each of these attributes is computed using something fundamental to the way the song or piece of music "sounds" to the listener. So, these characterisitics where chosen to be the basis of "similiarity" for defining triplets of anchor input, positive input (i.e., a similar input to the anchor class), and negative input (i.e., a dissimilar input to the anchor class). 

In the notebook "MusicInfoAnalysisandMr.Brightside.ipynb," we use the previously mentioned song attributes to define a similarity score which we employ to measure how similar each song is to the song "Mr. Brightside" by the Killers. However, this process could be repeated for any of the over 50,000 songs in the data set. An xgboost regressor model is created to predict how "similiar" a given song is to "Mr.Brightside" (and the performance is rather good with a low RMSE). This is an interesting implementation of not using raw audio or deep learning to come up with a measure of similarity. However, that was not the overall goal of this project. The important takeaway from this notebook is the "X_vector" label that was created from the six metadata attributes which we are essentially having define each song in the data set. 

In the notebook "K-ClusteringMusicInfo3Clusters.ipynb," we import the data used in the previously mentioned notebook and we use sklearn.cluster's KMeans to create 3 cluster labels for all the songs in the data set. The song data and the cluster label are then saved as a csv file. These three clusters are meant to yield the triples (anchor, positive, negative). The idea being that we could select any of the over 50,000 songs as the anchor, select another song belonging to the anchor's same cluster for the positive input, and another song from one of the other two clusters as its negative input. 

In the notebook, "IterativeLearning_3clusters.ipynb," we import the data with the cluster label and reproduce the "X_vector" label. We create multiple shuffled batches of the data set of size 500 as well as a testing set. We iteratively apply a partial fit of an SGD classifier. Each partial fit is evaluated and the results (accuracy, f1 score, precision, and recall) do slightly fluctuate but are all very high. It is possible that overfitting could be at play. 

In the notebook, "BatchTraining_SGD_3clusters_savedmodels.ipynb," we essentially repeat what we did in "IterativeLearning_3clusters.ipynb" except the way the data set is broken up, the number of epochs used, and the way batches are formed are all slightly different. Similarly, in this version, models are saved as external files ("model_data_id.sav", "model_data_id2.sav", "model_data_id3.sav", and "model_data_id4.sav"). The results were very similar to the previous notebook and the danger of overfitting still looms. The idea behind these last two notebooks is to define a function that would be able to predict what cluster a given song with its metadata scraped would belong to and thus find an easier way to generate triplets. 

In conclusion, we ran out of time to implement what we came up with in these notebooks for triplet loss, but this is something that could be applied in the future when training a siamese network seeking to measure music similarity.  